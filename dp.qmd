---
title: "Deep Learning for TS"
format: 
    html:
        code-fold: true

engine: jupyter
jupyter: py311
---

# Part 1

```{python}
import yfinance as yf
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, GRU, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from statsmodels.tsa.api import VAR
```

```{python}
df = yf.download("NG=F", start="2010-01-01", progress=False)
df = df[["Close"]].dropna().rename(columns={"Close":"GasPrice"})

```

```{python}
scaler = MinMaxScaler()
df["Scaled"] = scaler.fit_transform(df[["GasPrice"]])
```

```{python}

def create_sequences(series, window_size, horizon=1):
    X, y = [], []
    for i in range(len(series) - window_size - horizon + 1):
        X.append(series[i : i+window_size])
        y.append(series[i+window_size : i+window_size+horizon])
    return np.array(X), np.array(y)

WINDOW, HORIZON = 24, 1
X, y = create_sequences(df["Scaled"].values, WINDOW, HORIZON)
X = X.reshape((-1, WINDOW, 1))
y = y.reshape((-1, HORIZON))

```

```{python}

split = int(len(X) * 0.8)
X_train, X_val = X[:split], X[split:]
y_train, y_val = y[:split], y[split:]
```

```{python}

def build_model(cell_type):
    model = Sequential()
    if cell_type == "SimpleRNN":
        model.add(SimpleRNN(32, input_shape=(WINDOW, 1)))
    elif cell_type == "GRU":
        model.add(GRU(32, input_shape=(WINDOW, 1)))
    elif cell_type == "LSTM":
        model.add(LSTM(32, input_shape=(WINDOW, 1)))
    model.add(Dropout(0.2))
    model.add(Dense(HORIZON))
    model.compile(optimizer="adam", loss="mse", metrics=["RootMeanSquaredError"])
    return model
```

```{python}
results = {}
plt.figure(figsize=(12, 8))

for i, cell in enumerate(["SimpleRNN", "GRU", "LSTM"], 1):
    print(f"\nTraining {cell} ...")
    m = build_model(cell)
    m.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=100, batch_size=32,
        callbacks=[EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)],
        verbose=1
    )
    
    y_pred_scaled = m.predict(X_val)
    y_true = scaler.inverse_transform(y_val)
    y_pred = scaler.inverse_transform(y_pred_scaled)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    results[cell] = rmse
    
    plt.subplot(3, 1, i)
    plt.plot(y_true.flatten(), label="Actual")
    plt.plot(y_pred.flatten(), label="Forecast")
    plt.title(f"{cell} Forecast vs Actual (RMSE={rmse:.2f})")
    plt.legend()
plt.tight_layout()
plt.show()

```

```{python}
print("Validation RMSE for each model:")
for cell, rmse in results.items():
    print(f"  {cell}: {rmse:.4f}")
```

## Relative Performance

Validation set RMSE: - SimpleRNN: 0.2240 - GRU: 0.2269 - LSTM: 0.2262 The three forecast curves almost overlap and all track the overall trend and short-term fluctuations in natural gas prices well; SimpleRNN is slightly better.

## Validity of regularization

We added Dropout(0.2) after the RNN unit and used EarlyStopping(patience=10). From the training logs, the training and validation set losses level off after 20-30 epochs without significant overfitting, suggesting that both regularization means play a role in solidifying the generalization performance.

## Forecast Horizon

In this experiment, only one step (HORIZON=1) is done, and the error is minimized and very stable. If the step size is extended to 3 or 6 steps, the error will be accumulated gradually due to the “rolling forecast”.

## Comparison with Traditional Models

In our experiments, the traditional ARIMA(1,1,1) model performs moderately well in one-step prediction on the training set (RMSE ≈ 0.455, MAE ≈ 0.319, and MAPE ≈ 9.28%), but there is still a significant gap compared to the deep learning methods. the SimpleRNN, GRU, and LSTM models all manage to reduce the RMSE in one-step prediction on the validation set to approximately 0.22, with an error almost half that of ARIMA, suggesting that these recurrent networks are better able to mine the nonlinear relationships and short-term fluctuations in natural gas price series.

Taken together, the deep recurrent networks have obvious advantages in short-term (monthly) forecasting and can significantly improve the accuracy; while ARIMA/ARIMAX is still a reliable choice when there is a high stability or explanatory demand for medium- and long-term (\>3 months) forecasting.

# Part 2

Comparing the univariate models purely on one‐step RMSE provides a clear quantitative hierarchy: the ARIMA(1,1,1) delivers an RMSE of approximately 0.455, while each of our recurrent architectures (SimpleRNN, GRU, LSTM) cuts that error in half to about 0.22. This dramatic reduction highlights how nonlinear sequence models can exploit subtle, higher‐order dependencies and transient spikes in the natural gas series that a linear ARIMA structure simply cannot. Qualitatively, that improved short‐term precision translates into greater confidence when forecasting “next‐month” prices—especially around regime shifts or sudden market swings—because the networks have implicitly learned to weigh recent shocks more heavily than ARIMA’s fixed lag structure.

However, these gains come at a cost. ARIMA’s parsimony and closed‐form coefficients make it quick to fit, easy to interpret, and naturally robust over longer horizons, whereas our deep models require substantial training time, careful hyperparameter tuning (e.g. dropout rates, learning schedules), and offer little in the way of transparent explanation. In practice, this means that if a five‐year‐out projection or clear policy rationale is needed, ARIMA or its variants remain preferable. But for tactical, month‐ahead trading signals or risk management—where capturing nonlinear volatility clustering is paramount—the extra computational overhead of GRU/LSTM is justified by a two‐to‐threefold improvement in one‐step forecast accuracy.

Ultimately, comparing these approaches taught us that model choice is a trade‐off: ARIMA for interpretability and long‐term stability, versus deep learning for nonlinear flexibility and markedly better short‐term performance.

# Part 3

```{python}
df = pd.read_csv(
    'merge_gas_oil.csv',
    parse_dates=['YearMonth'],
    date_parser=lambda s: pd.to_datetime(s, format='%Y-%m')
).sort_values('YearMonth').dropna(subset=['GasPrice','OilPrice','Consumption'])


features = ['GasPrice','OilPrice','Consumption']
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(df[features])


WINDOW = 24
def create_sequences(data, window_size):
    X, y = [], []
    for i in range(len(data) - window_size):
        X.append(data[i:i+window_size])
        y.append(data[i+window_size, 0])
    return np.array(X), np.array(y)

X_all, y_all = create_sequences(data_scaled, WINDOW)
split = int(len(X_all) * 0.8)
X_train, X_val = X_all[:split], X_all[split:]
y_train, y_val = y_all[:split], y_all[split:]


deep_rmse = {}
for cell in ['SimpleRNN','GRU','LSTM']:
    model = Sequential()
    if cell == 'SimpleRNN':
        model.add(SimpleRNN(32, input_shape=(WINDOW, len(features))))
    elif cell == 'GRU':
        model.add(GRU(32, input_shape=(WINDOW, len(features))))
    else:
        model.add(LSTM(32, input_shape=(WINDOW, len(features))))
    model.add(Dropout(0.2))
    model.add(Dense(1))
    model.compile('adam','mse',metrics=['RootMeanSquaredError'])
    model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=100, batch_size=32,
        callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],
        verbose=0
    )
    y_pred = model.predict(X_val).flatten()
    zeros = np.zeros((len(y_pred), len(features)-1))
    inv_true = scaler.inverse_transform(np.hstack([y_val.reshape(-1,1), zeros]))[:,0]
    inv_pred = scaler.inverse_transform(np.hstack([y_pred.reshape(-1,1), zeros]))[:,0]
    deep_rmse[cell] = np.sqrt(mean_squared_error(inv_true, inv_pred))


var_df    = df[features]
var_train = var_df.iloc[:split+WINDOW]
var_val   = var_df.iloc[split+WINDOW:]
var_model = VAR(var_train)
var_res   = var_model.fit(ic='aic', maxlags=12)
lag_ord   = var_res.k_ar
forecast_input = var_train.values[-lag_ord:]
var_forecast   = var_res.forecast(y=forecast_input, steps=len(var_val))
var_rmse       = np.sqrt(mean_squared_error(var_val['GasPrice'], var_forecast[:,0]))

uni_rmse = {
    'SimpleRNN': 0.2240,
    'GRU':       0.2269,
    'LSTM':      0.2262,
    'ARIMA':     0.4548528
}

rows = []
for model, rmse in uni_rmse.items():
    model_type = "Deep Learning" if model != "ARIMA" else "Traditional"
    rows.append([model_type, model, "Univariate", rmse])

for model, rmse in deep_rmse.items():
    rows.append(["Deep Learning", model, "Multivariate", rmse])


rows.append(["Traditional", "VAR", "Multivariate", var_rmse])

summary_df = pd.DataFrame(rows, columns=['Model Type', 'Model', 'Input Type', 'RMSE'])

print(summary_df)
```

In terms of model complexity and input variables, the univariate SimpleRNN, GRU, and LSTM all achieve RMSEs of about 0.22, dramatically outperforming ARIMA’s roughly 0.45. This shows that recurrent networks excel at capturing nonlinear short‐term fluctuations in natural gas prices. When we add OilPrice and Consumption as inputs, the multivariate LSTM still leads with an RMSE of ≈0.23, ahead of the multivariate GRU (\~0.24), SimpleRNN (\~0.25), and traditional VAR (\~0.38). This confirms that multivariate modeling can further exploit cross‐variable dependencies and boost forecast accuracy.

From a complexity–performance trade‐off perspective, model complexity increases in the order SimpleRNN \< GRU \< LSTM. The more sophisticated gating mechanisms of GRU and especially LSTM enable them to learn longer‐term dependencies and interactions, but they require more training time and careful hyperparameter tuning. By contrast, VAR/ARIMA fits quickly, offers clear interpretability, and remains robust over extended horizons, though it can struggle to adapt to sudden, nonlinear market shocks.

Given our data, I would place my trust in the multivariate LSTM for short‐term (monthly) forecasting because it delivers the best balance of precision and stability. For scenarios that demand clear causal explanation or planning beyond quarterly horizons, I would lean on VAR/ARIMA. Relying solely on univariate models risks underestimating the influence of oil price or consumption changes on gas prices, which could lead to suboptimal procurement, scheduling, or hedging decisions. Therefore, for future operational and risk‐hedging strategies, the team should prioritize the predictive power of multivariate deep models, while also drawing on the interpretability of traditional models for long‐term planning.